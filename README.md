# Microllm

 just the bare basics to run inference on local hardware.

 currently working:
 - read_gguf.py
    Refactor not very good, it is faster and more compact but too verbose. 

todo:
- load tensors into model
- inference
